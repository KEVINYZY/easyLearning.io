#pragma once

#include <ATen/ATen.h>

// define constants like M_PI and C keywords for MSVC
#ifdef _MSC_VER
#define _USE_MATH_DEFINES
#include <ciso646>
#endif
#include <math.h>
#include <algorithm>
#include <numeric>

#define _AUTO_FIXME_ 1

using at::Tensor;
using at::Scalar;
using at::IntList;
using at::TensorList;
using at::TensorGeometry;

namespace express { namespace help {
  
// Helper functions for autogenerated code

Tensor not_implemented(const char* name);
Tensor maybe_multiply(const Tensor & t, const Scalar & s);
int64_t _safe_size(IntList sizes, int64_t dim);
Tensor norm_backward(const Tensor & grad, const Tensor & self, const Scalar & p_, const Tensor & norm);
Tensor norm_backward(Tensor grad, const Tensor & self, const Scalar & p_, Tensor norm, int64_t dim, bool keepdim);
Tensor permute_backwards(const Tensor & grad, IntList fwd_dims);
Tensor sum_backward(const Tensor & grad, IntList sizes, int64_t dim, bool keepdim);
Tensor reverse_dim(const Tensor& t, int64_t dim);
Tensor prod_safe_zeros_backward(const Tensor &grad, const Tensor& inp, int64_t dim);
Tensor prod_backward(const Tensor& grad, const Tensor& input, const Tensor& result);
Tensor prod_backward(Tensor grad, const Tensor& input, Tensor result, int64_t dim, bool keepdim);
Tensor sum_scan_exclusive(const Tensor& x, int64_t dim);
Tensor cumprod_backward(const Tensor &grad, const Tensor &input, int64_t dim);
Tensor cumsum_backward(const Tensor & x, int64_t dim);
Tensor unsqueeze_to(const Tensor & self, IntList sizes);
Tensor unsqueeze_to(const Tensor & self, int64_t dim, IntList sizes);
std::vector<Tensor> cat_tensors_backward(const Tensor & grad, const std::vector<std::vector<int64_t>> &sizes, int64_t dim);
Tensor mm_mat1_backward(const Tensor & grad, const Tensor & mat2, IntList sizes, IntList strides, const Scalar & alpha);
Tensor mm_mat2_backward(const Tensor & grad, const Tensor & mat1, IntList sizes, IntList strides, const Scalar & alpha);
Tensor renorm_backward(const Tensor & grad, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm);
Tensor sum_tensorlist(TensorList tl);
Tensor repeat_backward(Tensor grad, int64_t input_dims, IntList repeats);
Tensor select_backward_scalar(Tensor grad, const Tensor & input, const Tensor & value);
Tensor select_backward(Tensor grad, int64_t dim, Tensor indices, IntList sizes, bool keepdim);
Tensor trace_backward(const Tensor & grad, IntList sizes);
Tensor unfold_backward(const Tensor & grad, IntList input_sizes, int64_t dim, int64_t size, int64_t step);
Tensor var_backward(const Tensor & grad, const Tensor & self, bool unbiased);
Tensor var_backward(Tensor grad, const Tensor & self, int64_t dim, bool unbiased, bool keepdim);
Tensor masked_scatter_backward(const Tensor & grad, const Tensor & mask, IntList sizes);
Tensor potrf_backward(Tensor grad, bool upper, Tensor L);

#ifndef _AUTO_FIXME_
Tensor split_with_sizes_backward(const std::vector<torch::autograd::Variable> &grads,
                                 IntList split_sizes, int64_t dim, IntList sizes, const Type &type);
Tensor split_backward(const std::vector<torch::autograd::Variable> &grads,
                      int64_t split_size, int64_t dim, IntList sizes, const Type &type);
#endif
//
Tensor max_pool_double_backward(const Tensor & grad, const Tensor & indices, int dim);
Tensor glu_double_backward(const Tensor & grad, const Tensor & grad_output, const Tensor & input, int64_t dim);
Tensor glu_double_backward_grad_output(const Tensor & grad, const Tensor & input, int64_t dim);
Tensor kl_div_double_backward_grad_output(const Tensor & grad, const Tensor & input, const Tensor & target, bool size_average, bool reduce);
Tensor log_sigmoid_double_backward(const Tensor & grad, const Tensor & input);
Tensor softmax_double_backward(const Tensor & grad, const Tensor & grad_output, int dim, const Tensor & output);
Tensor log_softmax_double_backward(const Tensor & grad, const Tensor & grad_output, int dim, const Tensor & output);
Tensor l1_loss_double_backward_grad_output(const Tensor & grad, const Tensor & input, const Tensor & target, bool size_average, bool reduce);
Tensor smooth_l1_loss_double_backward(const Tensor & grad, const Tensor & input, const Tensor & target, bool size_average, bool reduce);
Tensor smooth_l1_loss_double_backward_grad_output(const Tensor & grad, const Tensor & grad_output, const Tensor & input, const Tensor & target, bool size_average, bool reduce);
Tensor diag_backward(const Tensor & grad, IntList input_sizes, int64_t diagonal);
Tensor mse_loss_double_backward(const Tensor & grad, const Tensor & input, bool size_average, bool reduce);
Tensor mse_loss_double_backward_grad_output(const Tensor & grad, const Tensor & grad_output, const Tensor & input, const Tensor & target, bool size_average, bool reduce);
Tensor soft_margin_loss_double_backward(const Tensor & grad, const Tensor & input, const Tensor & target, bool size_average, bool reduce);
Tensor soft_margin_loss_double_backward_grad_output(const Tensor & grad, const Tensor & grad_output, const Tensor & input, const Tensor & target, bool size_average, bool reduce);
Tensor softplus_double_backward(const Tensor & grad, const Tensor & input, Scalar beta, Scalar threshold);
Tensor as_strided_backward(const Tensor & grad, TensorGeometry base, IntList sizes, IntList strides, int64_t storage_offset);
std::tuple<Tensor, Tensor> atan2_backward(const Tensor& grad, const Tensor& self, const Tensor& other, std::array<bool, 2> output_mask);
std::tuple<Tensor, Tensor, Tensor> prelu_double_backward(
    const Tensor & mb_ggI,
    const Tensor & mb_ggW,
    const Tensor & mb_gO,
    const Tensor & input,
    const Tensor & weight,
    std::array<bool, 3> output_mask);


#ifndef _AUTO_FIXME_
Tensor svd_backward(const std::vector<torch::autograd::Variable> &grads, const Tensor& self,
          bool some, const Tensor& raw_u, const Tensor& sigma, const Tensor& raw_v);

Tensor det_backward(const Tensor & grad, const Tensor& self, const Tensor& det);
Tensor logdet_backward(const Tensor & grad, const Tensor& self, const Tensor& logdet);
Tensor slogdet_backward(const std::vector<torch::autograd::Variable> &grads,
                        const Tensor& self,
                        const Tensor& signdet, const Tensor& logabsdet);
#endif

std::tuple<Tensor, Tensor> trtrs_backward(
    const Tensor & grad_x, const Tensor & grad_m,
    const Tensor & b, const Tensor & a, const Tensor & x,
    const bool upper, const bool transpose, const bool unitriangular,
    std::array<bool, 2> output_mask);
Tensor fft_backward(const Tensor& self, const Tensor& grad, int64_t signal_ndim,
                    bool complex_input, bool complex_output,
                    bool inverse, IntList checked_signal_sizes,
                    bool normalized, bool onesided,
                    IntList output_sizes);
Tensor sum_exclude_dim1(const Tensor& to_sum, bool keepdim=true);
Tensor unsqueeze_dim1(const Tensor& src, const Tensor& target);
Tensor expand_as_dim1(const Tensor& src, const Tensor& target);
std::tuple<Tensor, Tensor, Tensor> batchnorm_double_backward(
    const Tensor & input,
    const Tensor & gamma,
    const Tensor & ggI,
    const Tensor & ggG,
    const Tensor & ggB,
    const Tensor & gO,
    const Tensor & running_mean,
    const Tensor & running_var,
    bool training,
    double eps,
    const Tensor & save_mean,
    const Tensor & save_std,
    std::array<bool,3> output_mask);
    std::tuple<Tensor, Tensor, Tensor> _trilinear_backward(const Tensor& grad_out, const Tensor& i1, const Tensor& i2, const Tensor& i3,
						       IntList expand1, IntList expand2, IntList expand3,
						       IntList sumdim, int64_t unroll_dim, std::array<bool, 3> grad_mask);

std::vector<std::vector<int64_t>> to_args_sizes(TensorList tensors);
template<int n> std::array<bool, n> build_mask() {
    std::array<bool, n> ret;
    for(size_t i = 0; i < ret.size(); i++) {
        ret[i] = true;
    }

    return ret;
}  

}} // namespace express::auto
